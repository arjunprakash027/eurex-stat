# eurex-stat

[Euraxess](https://euraxess.ec.europa.eu/jobs/search) is a European platform that connects researchers with job opportunities across universities, research institutions, and industry. It serves as a central hub for research-related career openings, covering multiple fields and countries. The platform is maintained by the European Commission and supports international mobility and career development for researchers.

This project is built around the goal of extracting and analyzing data from the Euraxess job portal to better understand academic hiring trends, demand across research fields, geographic distribution, and more.

## Quick Start – Run with Docker

You don’t need Python, Poetry, or any local dependencies.  
Simply pull the container from Docker Hub and run it; all outputs will land in a host-mounted folder of your choice.

```bash
docker run --rm -v "$(pwd)/data":/app/eurex_feature_engineering/output/transformed arjunrao123/eurex-stat:latest
```

* What happens:  
  1. The Scrapy spider crawls Euraxess and stores raw listings.  
  2. The processor cleans & enriches the data.  
  3. All CSV outputs appear in `./data` on your machine.

### Image details

* **Docker Hub:** <https://hub.docker.com/r/arjunrao123/eurex-stat>

The container removes itself after finishing (`--rm`), leaving only the data behind. Happy scraping!

## Project Overview

This repository is structured around three main goals:

1. **Data Collection**  
   Extract job listings from the Euraxess portal on a regular basis.

2. **Data Analysis** *(in progress)*  
   Apply statistical and exploratory methods to uncover patterns in the academic and research job market across Europe.

## Documentation

This repository includes multiple components, each with its own specific function. For technical details such as how the scrapers work or how to run the data pipeline, refer to the individual `README.md` files provided in the respective directories.

1) [Scrapper](https://github.com/arjunprakash027/eurex-stat/blob/main/eurex_scrapper/README.md)
2) [Feature Engineering](https://github.com/arjunprakash027/eurex-stat/blob/main/eurex_feature_engineering/README.md)

## Data Processing and Kaggle Workflow (`eurex_feature_engineering/main.py`)

The `eurex_feature_engineering/main.py` script is the central component for processing scraped data and managing the Kaggle dataset update workflow. It can operate in two modes: a local processing mode or a Kaggle-integrated mode.

### Running the Script

To run the script, navigate to the root directory of the repository and use:

```bash
python eurex_feature_engineering/main.py [arguments]
```

**Arguments:**
*   `--kaggle`: (Optional) A boolean flag. If provided, the script will execute the full Kaggle dataset update workflow.

### Workflow Description

#### With `--kaggle` Flag (Kaggle-Integrated Workflow)

When `python eurex_feature_engineering/main.py --kaggle` is executed:

1.  **Download from Kaggle**: Downloads the current version of the `arjunprakashrao/euraxess-full` dataset from Kaggle.
2.  **Transform Downloaded Kaggle Data**: The downloaded Kaggle data is processed through the feature engineering pipeline (defined in `eurex_feature_engineering/orchastrator.py` and its transformers).
3.  **Process New Daily Scraped Files**: The script looks for new "raw" data files in `eurex_feature_engineering/output/daily/` (these are assumed to be generated by `recent_date_vacancy_spider.py`, which should be run separately to collect new data). Each of these new daily files is also processed through the feature engineering pipeline.
4.  **Merge Data**: The transformed data from the downloaded Kaggle dataset is merged with the transformed data from the new daily scraped files.
5.  **Deduplication and Finalization**: Duplicates are handled (typically based on `job_link`, keeping the most recent entries), and the combined dataset is sorted. The result is saved locally to `eurex_feature_engineering/output/transformed/jobs_combined.csv`.
6.  **Upload to Kaggle**: This final, processed, and merged dataset is then uploaded back to Kaggle (`arjunprakashrao/euraxess-full`) as a new version.

#### Without `--kaggle` Flag (Local Processing Workflow)

When `python eurex_feature_engineering/main.py` is executed (without the `--kaggle` flag):

1.  **Load Local Base Data**: Attempts to load a previously existing local transformed dataset (e.g., `eurex_feature_engineering/output/transformed/jobs_combined.csv`) as the base.
2.  **Transform Local Base Data**: This local base data is re-processed through the current feature engineering pipeline to ensure consistency with the latest transformations.
3.  **Process New Daily Scraped Files**: Similar to the Kaggle workflow, new "raw" data files from `eurex_feature_engineering/output/daily/` are found and processed through the feature engineering pipeline.
4.  **Merge Data**: The transformed local base data is merged with the transformed newly scraped data.
5.  **Deduplication and Finalization**: Duplicates are handled, and the data is sorted. The result is saved locally to `eurex_feature_engineering/output/transformed/jobs_combined.csv`.
6.  **No Kaggle Interaction**: The script does not download from or upload to Kaggle in this mode.

### Prerequisites for `--kaggle` Workflow

For the Kaggle-integrated workflow (`--kaggle` flag) to function correctly:

1.  **Kaggle Python Package**: The `kaggle` library must be installed (it's included in `requirements.txt`).
2.  **Kaggle API Credentials**: A valid `kaggle.json` API token file must be correctly placed in your system (e.g., `~/.kaggle/kaggle.json` or environment variables `KAGGLE_USERNAME`/`KAGGLE_KEY`).
3.  **Scrapy Spider Output**: The `recent_date_vacancy_spider.py` should have been run independently to produce new "raw" data files in `eurex_feature_engineering/output/daily/`.

### Output

*   **Logs**: The script prints its progress to the console.
*   **Local Transformed CSV**: The primary output is `eurex_feature_engineering/output/transformed/jobs_combined.csv`, containing the processed and merged data.
*   **Kaggle Dataset Update (with `--kaggle` flag)**: If the `--kaggle` flag is used and the process is successful, a new version of the `arjunprakashrao/euraxess-full` dataset is created on Kaggle.
*   **Temporary Files (with `--kaggle` flag)**: Downloaded Kaggle data and temporary files for upload are stored in `kaggle_download_temp_fe/` (at the repository root).

### Euraxess Scraper (`eurex_scrapper/spiders/recent_date_vacancy_spider.py`)

The primary data collection is performed by the `recent_date_vacancy_spider.py` Scrapy spider. This spider extracts "raw" job listings from the Euraxess portal. It operates independently of the Kaggle workflow in `eurex_feature_engineering/main.py`.

#### Configuration

The spider's start date for scraping is determined by its configuration:

*   **`cutoff_date` (string)** in `eurex_scrapper/config.yaml`:
    *   Format: Can be `'today'`, `'DD-MM-YYYY'`, or `'YYYY-MM-DD'`.
    *   Usage: Defines the earliest posting date for jobs to be included in the scrape. Jobs posted on or after this date will be collected.

#### Running the Spider Directly

To run the spider for data collection:

1.  Navigate to the scraper's directory:
    ```bash
    cd eurex_scrapper
    ```
2.  Execute the crawl command:
    ```bash
    scrapy crawl recent_date_vacancy_spider
    ```

*   **Overriding Start Date**:
    You can override the `cutoff_date` from `config.yaml` by providing the `-a start_date` argument when running the spider directly. The date must be in `YYYY-MM-DD` format.
    ```bash
    scrapy crawl recent_date_vacancy_spider -a start_date=2023-10-01
    ```

#### Output

*   **Local CSV File**: The spider generates a "raw" CSV file of scraped jobs, named `jobs_YYYY-MM-DD.csv` (where `YYYY-MM-DD` is the effective start date of the scrape). By default, this file is saved into a directory structure *relative to where the spider is run*. If run from `eurex_scrapper/`, the path will be `eurex_scrapper/eurex_feature_engineering/output/daily/`. For `eurex_feature_engineering/main.py` to process these files, they should be made available in `eurex_feature_engineering/output/daily/` (relative to the project root).
